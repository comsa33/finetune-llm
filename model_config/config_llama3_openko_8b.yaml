model:
  name: "beomi/Llama-3-Open-Ko-8B"
  max_seq_length: 4096

dataset:
  repo_name: "comsa33/toeic_p5_qa_pair"
  num_proc: 4
  packing: False

tokenizer:
  padding_side: "right"

training:
  per_device_train_batch_size: 1
  gradient_accumulation_steps: 8
  warmup_steps: 5
  num_train_epochs: 20
  max_steps: 10000
  logging_steps: 50
  save_steps: 500
  learning_rate: 2.0e-4
  optim: "adamw_8bit"
  weight_decay: 0.01
  lr_scheduler_type: "cosine"
  seed: 123
  output_dir: "outputs"
  ddp_find_unused_parameters: False
